{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary packages\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.colors as mcolors\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /global/homes/d/dsmorrow/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import necessary packages for further word processing\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2020)\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ND_string2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>['allergi', 'percocet', 'lf', 'hemodialysi', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>['allergi', 'percocet', 'lf', 'lastnam', 'esrd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>['allergi', 'percocet', 'lf', 'micu', 'sle', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>['allergi', 'percocet', 'lf', 'hemodialysi', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>['allergi', 'percocet', 'namepattern', 'hemodi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0                                         ND_string2\n",
       "0          0  ['allergi', 'percocet', 'lf', 'hemodialysi', '...\n",
       "1          1  ['allergi', 'percocet', 'lf', 'lastnam', 'esrd...\n",
       "2          2  ['allergi', 'percocet', 'lf', 'micu', 'sle', '...\n",
       "3          3  ['allergi', 'percocet', 'lf', 'hemodialysi', '...\n",
       "4          4  ['allergi', 'percocet', 'namepattern', 'hemodi..."
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read in csv saved from python; see lemmatized_all.py\n",
    "#this csv includes text that only includes abbreviations, medications and misspellings\n",
    "NED = pd.read_csv(\"No_English_Dictionary.csv\", dtype=str)\n",
    "NED.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set(nltk.corpus.words.words())\n",
    "def preprocess2(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and token not in words:\n",
    "            result.append(token)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs2 = NED['ND_string2'].map(preprocess2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 aliskiren\n",
      "1 allergi\n",
      "2 amp\n",
      "3 anicter\n",
      "4 antibodi\n",
      "5 anticardiolipin\n",
      "6 anticoagul\n",
      "7 autoimmun\n",
      "8 baselin\n",
      "9 bp\n",
      "10 cerebrovascular\n"
     ]
    }
   ],
   "source": [
    "#create dictionary of words\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs2)\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter out words that appear in less than (15) documents\n",
    "#only keep the first 10000\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#go through each document and report words and occurrences using doc2box for token id and amount\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(111, 0.014824932283466754),\n",
      " (101, 0.015331008365657895),\n",
      " (60, 0.01701150571240441),\n",
      " (1, 0.017066553563346427),\n",
      " (63, 0.01720501358485213),\n",
      " (24, 0.017687606155084624),\n",
      " (95, 0.018922226688739562),\n",
      " (102, 0.019085287114207792),\n",
      " (46, 0.01946364199676075),\n",
      " (25, 0.01957405138039286),\n",
      " (79, 0.019680071704618662),\n",
      " (109, 0.019988752104761188),\n",
      " (113, 0.020086447203989666),\n",
      " (82, 0.020963733503304133),\n",
      " (64, 0.021228161663913055),\n",
      " (54, 0.02268044277953536),\n",
      " (97, 0.023498654611390484),\n",
      " (33, 0.0238130154015657),\n",
      " (35, 0.024037646095580328),\n",
      " (87, 0.024119027207548598),\n",
      " (23, 0.024691545880756817),\n",
      " (132, 0.02492167824234219),\n",
      " (49, 0.024993349927011913),\n",
      " (72, 0.02539866471739196),\n",
      " (39, 0.025463723567176487),\n",
      " (38, 0.027951286606202407),\n",
      " (43, 0.02799199777374769),\n",
      " (96, 0.029027967504815202),\n",
      " (17, 0.029874728760229143),\n",
      " (47, 0.030479058068940076),\n",
      " (32, 0.03106508874810856),\n",
      " (51, 0.03176116828983889),\n",
      " (130, 0.03183893374872267),\n",
      " (6, 0.03198683787419499),\n",
      " (107, 0.03198783596611507),\n",
      " (71, 0.03214187583371),\n",
      " (92, 0.032157122045695326),\n",
      " (122, 0.03336100763131531),\n",
      " (77, 0.033595782947015464),\n",
      " (115, 0.033875065572180306),\n",
      " (108, 0.0341303180177153),\n",
      " (75, 0.03426846993769091),\n",
      " (76, 0.03432796886240687),\n",
      " (3, 0.03436438099520517),\n",
      " (114, 0.03461815290290749),\n",
      " (94, 0.034649042957684235),\n",
      " (16, 0.0355307591311652),\n",
      " (67, 0.03607425862486609),\n",
      " (9, 0.03611190915393218),\n",
      " (73, 0.036172674224591236),\n",
      " (83, 0.036494448617122185),\n",
      " (45, 0.03682999217822448),\n",
      " (29, 0.0369302139282746),\n",
      " (100, 0.037196683351009445),\n",
      " (106, 0.03767464913893544),\n",
      " (118, 0.039157150706709935),\n",
      " (20, 0.03923557424356586),\n",
      " (134, 0.04015457409220834),\n",
      " (50, 0.0401970894724373),\n",
      " (127, 0.0403106188429683),\n",
      " (99, 0.04040249497461227),\n",
      " (74, 0.04041192836186238),\n",
      " (2, 0.04066278961201866),\n",
      " (26, 0.04112891814740434),\n",
      " (21, 0.04116239304553766),\n",
      " (126, 0.04170682054165949),\n",
      " (14, 0.042067386427457096),\n",
      " (22, 0.04284220243712706),\n",
      " (34, 0.043526748712588284),\n",
      " (8, 0.043573728755449924),\n",
      " (84, 0.04382538964134999),\n",
      " (133, 0.04412020425384864),\n",
      " (27, 0.04446596345318401),\n",
      " (91, 0.0447658769994441),\n",
      " (128, 0.044820451200347215),\n",
      " (110, 0.045037835798001816),\n",
      " (70, 0.04569116915464711),\n",
      " (41, 0.04574876395807376),\n",
      " (19, 0.046258716644510485),\n",
      " (40, 0.04636396788826997),\n",
      " (93, 0.047479523768077694),\n",
      " (13, 0.04758060678887932),\n",
      " (116, 0.04889730363736023),\n",
      " (55, 0.049713710713355126),\n",
      " (88, 0.050198593733466176),\n",
      " (56, 0.05026039264976622),\n",
      " (65, 0.050575981640691624),\n",
      " (7, 0.0513472614111277),\n",
      " (30, 0.05156891524620345),\n",
      " (90, 0.0531031117468824),\n",
      " (42, 0.05381309539997933),\n",
      " (61, 0.05397624036075777),\n",
      " (85, 0.0543239902786904),\n",
      " (124, 0.05437303831425417),\n",
      " (112, 0.05501005624560194),\n",
      " (4, 0.05698276706722745),\n",
      " (59, 0.05709886397478223),\n",
      " (62, 0.05733569462392099),\n",
      " (10, 0.05752629282264059),\n",
      " (31, 0.06002083617460713),\n",
      " (36, 0.061416475507631976),\n",
      " (5, 0.06291799683026596),\n",
      " (52, 0.06331134554927675),\n",
      " (121, 0.06449213553304342),\n",
      " (81, 0.06617115822300511),\n",
      " (28, 0.07168296125750429),\n",
      " (44, 0.07402179646721181),\n",
      " (11, 0.07560263309396928),\n",
      " (66, 0.0756253463614495),\n",
      " (58, 0.07809700811770613),\n",
      " (129, 0.07923914057924034),\n",
      " (12, 0.08613527505149167),\n",
      " (18, 0.08715916291745791),\n",
      " (78, 0.08750138747696946),\n",
      " (123, 0.08782862745736908),\n",
      " (119, 0.10070780361521622),\n",
      " (68, 0.10182297919555962),\n",
      " (53, 0.10507844611169848),\n",
      " (86, 0.11570959332467604),\n",
      " (120, 0.11670685011957257),\n",
      " (37, 0.12191632706187684),\n",
      " (69, 0.12604492647368518),\n",
      " (103, 0.12993940136021975),\n",
      " (98, 0.13029019772387318),\n",
      " (104, 0.14665202518667894),\n",
      " (117, 0.14705358489214884),\n",
      " (48, 0.15499375165438825),\n",
      " (57, 0.17417645776786364),\n",
      " (131, 0.1861725180914012),\n",
      " (0, 0.19464954968151402),\n",
      " (15, 0.21526905649447625),\n",
      " (89, 0.24549574522396655),\n",
      " (105, 0.2922857138572846),\n",
      " (125, 0.30829414648620185),\n",
      " (80, 0.4328227200762227)]\n"
     ]
    }
   ],
   "source": [
    "#determine the TF-IDF scores or weight of a word within documents\n",
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "from pprint import pprint\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(sorted(doc, key = lambda x: x[1]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mg ,transderm ,qwed ,nifedipin ,clonidin ,aliskiren ,wednesday ,hydralazin ,hd ,sle\n"
     ]
    }
   ],
   "source": [
    "#top ten weighted words\n",
    "print(dictionary[80] ,\",\" + dictionary[125] ,\",\" + dictionary[105] ,\",\" + dictionary[89] ,\",\" + dictionary[15] ,\",\" + dictionary[0] ,\",\" + dictionary[131],\",\" + dictionary[57],\",\" + dictionary[48],\",\" + dictionary[117])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#these weighted words have alot to do with medications\n",
    "#mg = milligrams\n",
    "#transderm = transdermal - relating to or denoting the application of a medicine or drug through the skin\n",
    "#qwed = \n",
    "#nifedipine = Antihypertensive drug and Calcium channel blocker\n",
    "#clonidine = Sedative and Antihypertensive drug\n",
    "#aliskiren = Antihypertensive drug\n",
    "#hd\n",
    "#sle \n",
    "#high blood pressure treatment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['labetalol (0.361)', 'mg (0.316)', 'prednison (0.240)', 'nitropast (0.234)', 'nicardipin (0.185)']\n"
     ]
    }
   ],
   "source": [
    "#top weighted words in document 30\n",
    "tf_obj = corpus_tfidf[30]\n",
    "sorted(tf_obj, key=lambda x: x[1], reverse=True)\n",
    "n_terms = 5\n",
    "\n",
    "top_terms = []\n",
    "for obj in sorted(tf_obj, key=lambda x: x[1], reverse=True)[:n_terms]:\n",
    "    top_terms.append(\"{0:s} ({1:01.03f})\".format(dictionary[obj[0]], obj[1]))\n",
    "\n",
    "print(top_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['labetalol (0.338)', 'sbo (0.330)', 'hd (0.277)', 'sle (0.263)', 'ml (0.251)']\n"
     ]
    }
   ],
   "source": [
    "#top weighted words in document 747\n",
    "tf_obj = corpus_tfidf[747]\n",
    "sorted(tf_obj, key=lambda x: x[1], reverse=True)\n",
    "n_terms = 5\n",
    "\n",
    "top_terms = []\n",
    "for obj in sorted(tf_obj, key=lambda x: x[1], reverse=True)[:n_terms]:\n",
    "    top_terms.append(\"{0:s} ({1:01.03f})\".format(dictionary[obj[0]], obj[1]))\n",
    "\n",
    "print(top_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['med (0.409)', 'zofran (0.399)', 'pt (0.302)', 'pd (0.296)', 'orderd (0.273)']\n"
     ]
    }
   ],
   "source": [
    "#top weighted words in document 382\n",
    "tf_obj = corpus_tfidf[382]\n",
    "sorted(tf_obj, key=lambda x: x[1], reverse=True)\n",
    "n_terms = 5\n",
    "\n",
    "top_terms = []\n",
    "for obj in sorted(tf_obj, key=lambda x: x[1], reverse=True)[:n_terms]:\n",
    "    top_terms.append(\"{0:s} ({1:01.03f})\".format(dictionary[obj[0]], obj[1]))\n",
    "\n",
    "print(top_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-intern]",
   "language": "python",
   "name": "conda-env-.conda-intern-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
