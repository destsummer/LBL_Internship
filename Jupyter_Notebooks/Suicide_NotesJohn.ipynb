{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary packages\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.colors as mcolors\n",
    "import re\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "dateparse = lambda dates: [pd.datetime.strptime(d, '%Y-%m-%d %H:%M:%S') for d in dates]\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /global/homes/d/dsmorrow/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import necessary packages for further word processing\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2020)\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to perform lemmatize and stem preprocessing steps on the data set.\n",
    "def lemmatize_stemming(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 2:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PID</th>\n",
       "      <th>TIMESTAMP</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000000000847_0000</td>\n",
       "      <td>200908191542_2666</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000000000871_0000</td>\n",
       "      <td>200908241036_5644</td>\n",
       "      <td>have ever know. But I can't take it any more, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000000000357_0000</td>\n",
       "      <td>200809091750_3220</td>\n",
       "      <td>( Give to Newspaper)                        ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000000000151_0000</td>\n",
       "      <td>200809091738_2108</td>\n",
       "      <td>Police:\\n          Just a few words is only s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000000000320_0000</td>\n",
       "      <td>200907231445_0958</td>\n",
       "      <td>To Little Ann\\nRemember me Darling. To daddy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1315</th>\n",
       "      <td>000000000077_0000</td>\n",
       "      <td>200908061525_4746</td>\n",
       "      <td>Landlord - please if this doesnt act call my S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1316</th>\n",
       "      <td>000000000223_0000</td>\n",
       "      <td>200809091808_0160</td>\n",
       "      <td>Jane: The only thing you never called me was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1317</th>\n",
       "      <td>000000000271_0000</td>\n",
       "      <td>200809091809_5363</td>\n",
       "      <td>John --\\nThis is a dirty trick to play on yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1318</th>\n",
       "      <td>000000000564_0000</td>\n",
       "      <td>200812181836_2019</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1319</th>\n",
       "      <td>000000000528_0000</td>\n",
       "      <td>200909011253_2589</td>\n",
       "      <td>Honey:-\\nGive John Johnson whatever you can \"...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1320 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    PID          TIMESTAMP  \\\n",
       "0     000000000847_0000  200908191542_2666   \n",
       "1     000000000871_0000  200908241036_5644   \n",
       "2     000000000357_0000  200809091750_3220   \n",
       "3     000000000151_0000  200809091738_2108   \n",
       "4     000000000320_0000  200907231445_0958   \n",
       "...                 ...                ...   \n",
       "1315  000000000077_0000  200908061525_4746   \n",
       "1316  000000000223_0000  200809091808_0160   \n",
       "1317  000000000271_0000  200809091809_5363   \n",
       "1318  000000000564_0000  200812181836_2019   \n",
       "1319  000000000528_0000  200909011253_2589   \n",
       "\n",
       "                                                   TEXT  \n",
       "0                                                   ...  \n",
       "1     have ever know. But I can't take it any more, ...  \n",
       "2       ( Give to Newspaper)                        ...  \n",
       "3      Police:\\n          Just a few words is only s...  \n",
       "4      To Little Ann\\nRemember me Darling. To daddy ...  \n",
       "...                                                 ...  \n",
       "1315  Landlord - please if this doesnt act call my S...  \n",
       "1316   Jane: The only thing you never called me was ...  \n",
       "1317    John --\\nThis is a dirty trick to play on yo...  \n",
       "1318                                                ...  \n",
       "1319   Honey:-\\nGive John Johnson whatever you can \"...  \n",
       "\n",
       "[1320 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suicide_notes = pd.read_csv(\"suicide_notes.csv\")\n",
    "suicide_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove line breaks\n",
    "suicide_notes['TEXT'] = suicide_notes['TEXT'].map(lambda x: re.sub('\\n+', ' ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove these special characters found in many notes\n",
    "suicide_notes['TEXT'] = suicide_notes['TEXT'].map(lambda x: re.sub('\\xa0+', ' ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the name johnson\n",
    "suicide_notes['TEXT'] = suicide_notes['TEXT'].map(lambda x: re.sub('Johnson', ' ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the name john\n",
    "suicide_notes['TEXT'] = suicide_notes['TEXT'].map(lambda x: re.sub('John', ' ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the name jane\n",
    "suicide_notes['TEXT'] = suicide_notes['TEXT'].map(lambda x: re.sub('Jane', ' ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "suicide_notes['text_2'] = suicide_notes['TEXT'].map(lambda x: re.sub('[^A-Za-z0-9]+', ' ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  3:30 PM Saturday  Sweet lovely-I fought I cannot even hold my urine!  So I mercy on you and end it all   Love to you + (M)   Bill '"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suicide_notes['TEXT'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "suicide_notes['tokenized_text'] = suicide_notes['text_2'].apply(word_tokenize) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PID</th>\n",
       "      <th>TIMESTAMP</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>tokenized_text</th>\n",
       "      <th>text_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000000000847_0000</td>\n",
       "      <td>200908191542_2666</td>\n",
       "      <td>3:30 PM Saturday  Sweet lovely-I fought I ca...</td>\n",
       "      <td>[3, 30, PM, Saturday, Sweet, lovely, I, fought...</td>\n",
       "      <td>3 30 PM Saturday Sweet lovely I fought I cann...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000000000871_0000</td>\n",
       "      <td>200908241036_5644</td>\n",
       "      <td>have ever know. But I can't take it any more, ...</td>\n",
       "      <td>[have, ever, know, But, I, can, t, take, it, a...</td>\n",
       "      <td>have ever know But I can t take it any more pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000000000357_0000</td>\n",
       "      <td>200809091750_3220</td>\n",
       "      <td>( Give to Newspaper) Mrs. J. J.     3333 Bur...</td>\n",
       "      <td>[Give, to, Newspaper, Mrs, J, J, 3333, Burnet,...</td>\n",
       "      <td>Give to Newspaper Mrs J J 3333 Burnet Ave Cin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000000000151_0000</td>\n",
       "      <td>200809091738_2108</td>\n",
       "      <td>Police:   Just a few words is only sufficient...</td>\n",
       "      <td>[Police, Just, a, few, words, is, only, suffic...</td>\n",
       "      <td>Police Just a few words is only sufficient I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000000000320_0000</td>\n",
       "      <td>200907231445_0958</td>\n",
       "      <td>To Little Ann Remember me Darling. To daddy y...</td>\n",
       "      <td>[To, Little, Ann, Remember, me, Darling, To, d...</td>\n",
       "      <td>To Little Ann Remember me Darling To daddy yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1315</th>\n",
       "      <td>000000000077_0000</td>\n",
       "      <td>200908061525_4746</td>\n",
       "      <td>Landlord - please if this doesnt act call my S...</td>\n",
       "      <td>[Landlord, please, if, this, doesnt, act, call...</td>\n",
       "      <td>Landlord please if this doesnt act call my Sis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1316</th>\n",
       "      <td>000000000223_0000</td>\n",
       "      <td>200809091808_0160</td>\n",
       "      <td>: The only thing you never called me was cra...</td>\n",
       "      <td>[The, only, thing, you, never, called, me, was...</td>\n",
       "      <td>The only thing you never called me was crazy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1317</th>\n",
       "      <td>000000000271_0000</td>\n",
       "      <td>200809091809_5363</td>\n",
       "      <td>-- This is a dirty trick to play on you. I...</td>\n",
       "      <td>[This, is, a, dirty, trick, to, play, on, you,...</td>\n",
       "      <td>This is a dirty trick to play on you I don t ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1318</th>\n",
       "      <td>000000000564_0000</td>\n",
       "      <td>200812181836_2019</td>\n",
       "      <td>xxxxxx 1-01-01 My dear Girls: When I see the ...</td>\n",
       "      <td>[xxxxxx, 1, 01, 01, My, dear, Girls, When, I, ...</td>\n",
       "      <td>xxxxxx 1 01 01 My dear Girls When I see the e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1319</th>\n",
       "      <td>000000000528_0000</td>\n",
       "      <td>200909011253_2589</td>\n",
       "      <td>Honey:- Give     whatever you can \"A Swell ma...</td>\n",
       "      <td>[Honey, Give, whatever, you, can, A, Swell, ma...</td>\n",
       "      <td>Honey Give whatever you can A Swell man Give ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1320 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    PID          TIMESTAMP  \\\n",
       "0     000000000847_0000  200908191542_2666   \n",
       "1     000000000871_0000  200908241036_5644   \n",
       "2     000000000357_0000  200809091750_3220   \n",
       "3     000000000151_0000  200809091738_2108   \n",
       "4     000000000320_0000  200907231445_0958   \n",
       "...                 ...                ...   \n",
       "1315  000000000077_0000  200908061525_4746   \n",
       "1316  000000000223_0000  200809091808_0160   \n",
       "1317  000000000271_0000  200809091809_5363   \n",
       "1318  000000000564_0000  200812181836_2019   \n",
       "1319  000000000528_0000  200909011253_2589   \n",
       "\n",
       "                                                   TEXT  \\\n",
       "0       3:30 PM Saturday  Sweet lovely-I fought I ca...   \n",
       "1     have ever know. But I can't take it any more, ...   \n",
       "2       ( Give to Newspaper) Mrs. J. J.     3333 Bur...   \n",
       "3      Police:   Just a few words is only sufficient...   \n",
       "4      To Little Ann Remember me Darling. To daddy y...   \n",
       "...                                                 ...   \n",
       "1315  Landlord - please if this doesnt act call my S...   \n",
       "1316    : The only thing you never called me was cra...   \n",
       "1317      -- This is a dirty trick to play on you. I...   \n",
       "1318   xxxxxx 1-01-01 My dear Girls: When I see the ...   \n",
       "1319   Honey:- Give     whatever you can \"A Swell ma...   \n",
       "\n",
       "                                         tokenized_text  \\\n",
       "0     [3, 30, PM, Saturday, Sweet, lovely, I, fought...   \n",
       "1     [have, ever, know, But, I, can, t, take, it, a...   \n",
       "2     [Give, to, Newspaper, Mrs, J, J, 3333, Burnet,...   \n",
       "3     [Police, Just, a, few, words, is, only, suffic...   \n",
       "4     [To, Little, Ann, Remember, me, Darling, To, d...   \n",
       "...                                                 ...   \n",
       "1315  [Landlord, please, if, this, doesnt, act, call...   \n",
       "1316  [The, only, thing, you, never, called, me, was...   \n",
       "1317  [This, is, a, dirty, trick, to, play, on, you,...   \n",
       "1318  [xxxxxx, 1, 01, 01, My, dear, Girls, When, I, ...   \n",
       "1319  [Honey, Give, whatever, you, can, A, Swell, ma...   \n",
       "\n",
       "                                                 text_2  \n",
       "0      3 30 PM Saturday Sweet lovely I fought I cann...  \n",
       "1     have ever know But I can t take it any more pl...  \n",
       "2      Give to Newspaper Mrs J J 3333 Burnet Ave Cin...  \n",
       "3      Police Just a few words is only sufficient I ...  \n",
       "4      To Little Ann Remember me Darling To daddy yo...  \n",
       "...                                                 ...  \n",
       "1315  Landlord please if this doesnt act call my Sis...  \n",
       "1316   The only thing you never called me was crazy ...  \n",
       "1317   This is a dirty trick to play on you I don t ...  \n",
       "1318   xxxxxx 1 01 01 My dear Girls When I see the e...  \n",
       "1319   Honey Give whatever you can A Swell man Give ...  \n",
       "\n",
       "[1320 rows x 5 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suicide_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "run1 = suicide_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [saturday, sweet, love, fight, hold, urin, mer...\n",
       "1       [know, forgiv, know, key, safti, deposit, box,...\n",
       "2       [newspap, mr, burnet, ave, cincinnati, answer,...\n",
       "3       [polic, word, suffici, guess, let, know, sick,...\n",
       "4       [littl, ann, rememb, darl, daddi, sweetest, gi...\n",
       "                              ...                        \n",
       "1315    [landlord, doesnt, act, sister, mr, smith, bur...\n",
       "1316                           [thing, call, crazi, love]\n",
       "1317    [dirti, trick, play, want, live, longer, look,...\n",
       "1318    [xxxxxx, dear, girl, end, life, ambit, complet...\n",
       "1319    [honey, swell, man, good, kid, brown, suit, wa...\n",
       "Name: TEXT, Length: 1320, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use preprocess function \n",
    "processed_SN = run1['TEXT'].map(preprocess)\n",
    "processed_SN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 end\n",
      "1 fight\n",
      "2 hold\n",
      "3 love\n",
      "4 merci\n",
      "5 saturday\n",
      "6 sweet\n",
      "7 urin\n",
      "8 bank\n",
      "9 box\n",
      "10 brown\n"
     ]
    }
   ],
   "source": [
    "#create dictionary of words\n",
    "dictionary = gensim.corpora.Dictionary(processed_SN)\n",
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_SN]\n",
    "#bow_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 0.16144357926769642),\n",
      " (0, 0.22798068660139068),\n",
      " (2, 0.29629699808154264),\n",
      " (6, 0.32239547925320505),\n",
      " (1, 0.3408821615096014),\n",
      " (4, 0.35586609994285223),\n",
      " (5, 0.41962823084727346),\n",
      " (7, 0.5579434020462551)]\n"
     ]
    }
   ],
   "source": [
    "#determine the TF-IDF scores or weight of a word within a document\n",
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "from pprint import pprint\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(sorted(doc, key = lambda x: x[1]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "urin ,saturday ,merci ,fight ,sweet ,hold ,end ,love\n"
     ]
    }
   ],
   "source": [
    "#top 8 weighted words\n",
    "#why are these the only weighted words?\n",
    "print(dictionary[7] ,\",\" + dictionary[5] ,\",\" + dictionary[4] ,\",\" + dictionary[1] ,\",\" + dictionary[6] ,\",\" + dictionary[2] ,\",\" + dictionary[0] ,\",\" + dictionary[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train LDA model using BOW, 10 topics, 10 passes\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=bow_corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=10, \n",
    "                                           random_state=100,\n",
    "                                           passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.057*\"cincinnati\" + 0.055*\"burnet\" + 0.046*\"ave\" + 0.029*\"ohio\" + '\n",
      "  '0.016*\"mr\" + 0.013*\"notifi\" + 0.011*\"hospit\" + 0.011*\"smith\" + '\n",
      "  '0.010*\"phone\" + 0.009*\"sign\"'),\n",
      " (1,\n",
      "  '0.034*\"love\" + 0.025*\"good\" + 0.021*\"know\" + 0.018*\"way\" + 0.015*\"god\" + '\n",
      "  '0.014*\"sorri\" + 0.014*\"want\" + 0.014*\"dear\" + 0.014*\"thing\" + '\n",
      "  '0.012*\"think\"'),\n",
      " (2,\n",
      "  '0.011*\"blame\" + 0.008*\"tire\" + 0.008*\"sheriff\" + 0.007*\"leav\" + '\n",
      "  '0.007*\"envelop\" + 0.007*\"pat\" + 0.006*\"take\" + 0.006*\"concern\" + '\n",
      "  '0.006*\"son\" + 0.006*\"collect\"'),\n",
      " (3,\n",
      "  '0.012*\"smith\" + 0.011*\"money\" + 0.009*\"car\" + 0.009*\"wish\" + 0.009*\"pay\" + '\n",
      "  '0.008*\"know\" + 0.007*\"mr\" + 0.007*\"bodi\" + 0.007*\"mari\" + 0.006*\"check\"'),\n",
      " (4,\n",
      "  '0.025*\"love\" + 0.016*\"forgiv\" + 0.015*\"good\" + 0.013*\"dear\" + 0.011*\"god\" + '\n",
      "  '0.010*\"way\" + 0.010*\"want\" + 0.008*\"pain\" + 0.007*\"wife\" + 0.007*\"longer\"'),\n",
      " (5,\n",
      "  '0.021*\"love\" + 0.010*\"day\" + 0.010*\"dear\" + 0.010*\"good\" + 0.010*\"life\" + '\n",
      "  '0.009*\"get\" + 0.008*\"money\" + 0.008*\"time\" + 0.007*\"thank\" + 0.006*\"bye\"'),\n",
      " (6,\n",
      "  '0.020*\"love\" + 0.012*\"good\" + 0.012*\"know\" + 0.011*\"want\" + 0.010*\"dear\" + '\n",
      "  '0.007*\"thing\" + 0.007*\"children\" + 0.007*\"hope\" + 0.007*\"mother\" + '\n",
      "  '0.006*\"end\"'),\n",
      " (7,\n",
      "  '0.017*\"cincinnati\" + 0.012*\"mari\" + 0.011*\"burnet\" + 0.010*\"go\" + '\n",
      "  '0.009*\"ave\" + 0.009*\"smith\" + 0.009*\"leav\" + 0.009*\"want\" + 0.008*\"hope\" + '\n",
      "  '0.008*\"get\"'),\n",
      " (8,\n",
      "  '0.034*\"love\" + 0.021*\"know\" + 0.021*\"want\" + 0.013*\"life\" + 0.011*\"forgiv\" '\n",
      "  '+ 0.011*\"tri\" + 0.011*\"live\" + 0.011*\"leav\" + 0.010*\"tell\" + '\n",
      "  '0.010*\"mother\"'),\n",
      " (9,\n",
      "  '0.029*\"mari\" + 0.021*\"smith\" + 0.016*\"cincinnati\" + 0.015*\"know\" + '\n",
      "  '0.012*\"ohio\" + 0.011*\"love\" + 0.011*\"pay\" + 0.010*\"dear\" + 0.009*\"januari\" '\n",
      "  '+ 0.008*\"life\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keywords in the 10 topics using the BOW corpus and 10 passes\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train LDA model using BOW, 20 topics, 10 passes\n",
    "lda_model2 = gensim.models.ldamodel.LdaModel(corpus=bow_corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.084*\"cincinnati\" + 0.069*\"burnet\" + 0.059*\"ave\" + 0.045*\"ohio\" + '\n",
      "  '0.018*\"mr\" + 0.017*\"smith\" + 0.016*\"hospit\" + 0.014*\"sign\" + 0.013*\"notifi\" '\n",
      "  '+ 0.013*\"children\"'),\n",
      " (1,\n",
      "  '0.029*\"know\" + 0.027*\"love\" + 0.025*\"good\" + 0.023*\"god\" + 0.020*\"way\" + '\n",
      "  '0.013*\"mother\" + 0.013*\"best\" + 0.013*\"bless\" + 0.012*\"thing\" + '\n",
      "  '0.012*\"sorri\"'),\n",
      " (2,\n",
      "  '0.023*\"blame\" + 0.013*\"sign\" + 0.012*\"know\" + 0.012*\"concern\" + '\n",
      "  '0.010*\"take\" + 0.009*\"suicid\" + 0.008*\"tire\" + 0.008*\"time\" + '\n",
      "  '0.008*\"author\" + 0.008*\"year\"'),\n",
      " (3,\n",
      "  '0.018*\"care\" + 0.016*\"money\" + 0.011*\"car\" + 0.011*\"pay\" + 0.010*\"check\" + '\n",
      "  '0.010*\"know\" + 0.009*\"work\" + 0.008*\"like\" + 0.007*\"happi\" + 0.007*\"live\"'),\n",
      " (4,\n",
      "  '0.044*\"love\" + 0.029*\"good\" + 0.026*\"forgiv\" + 0.023*\"dear\" + 0.014*\"bye\" + '\n",
      "  '0.013*\"god\" + 0.013*\"way\" + 0.012*\"want\" + 0.011*\"mom\" + 0.011*\"go\"'),\n",
      " (5,\n",
      "  '0.021*\"love\" + 0.012*\"mari\" + 0.011*\"good\" + 0.011*\"dear\" + 0.011*\"day\" + '\n",
      "  '0.010*\"thank\" + 0.009*\"live\" + 0.008*\"money\" + 0.008*\"leav\" + 0.008*\"time\"'),\n",
      " (6,\n",
      "  '0.025*\"love\" + 0.014*\"know\" + 0.013*\"good\" + 0.012*\"want\" + 0.010*\"go\" + '\n",
      "  '0.010*\"thing\" + 0.009*\"hope\" + 0.008*\"care\" + 0.008*\"end\" + 0.008*\"time\"'),\n",
      " (7,\n",
      "  '0.012*\"cincinnati\" + 0.012*\"want\" + 0.012*\"mari\" + 0.010*\"go\" + '\n",
      "  '0.009*\"leav\" + 0.009*\"know\" + 0.008*\"good\" + 0.008*\"smith\" + 0.008*\"ave\" + '\n",
      "  '0.008*\"burnet\"'),\n",
      " (8,\n",
      "  '0.030*\"love\" + 0.015*\"know\" + 0.015*\"leav\" + 0.015*\"want\" + 0.013*\"life\" + '\n",
      "  '0.013*\"tri\" + 0.011*\"like\" + 0.011*\"live\" + 0.010*\"time\" + 0.010*\"help\"'),\n",
      " (9,\n",
      "  '0.031*\"mari\" + 0.014*\"love\" + 0.014*\"hope\" + 0.013*\"know\" + 0.011*\"smith\" + '\n",
      "  '0.010*\"dear\" + 0.009*\"wife\" + 0.009*\"cincinnati\" + 0.009*\"time\" + '\n",
      "  '0.008*\"good\"'),\n",
      " (10,\n",
      "  '0.023*\"smith\" + 0.022*\"burnet\" + 0.019*\"cincinnati\" + 0.018*\"mr\" + '\n",
      "  '0.017*\"ave\" + 0.017*\"pay\" + 0.013*\"money\" + 0.010*\"wish\" + 0.010*\"owe\" + '\n",
      "  '0.009*\"want\"'),\n",
      " (11,\n",
      "  '0.015*\"want\" + 0.013*\"good\" + 0.009*\"hope\" + 0.009*\"live\" + 0.009*\"like\" + '\n",
      "  '0.008*\"mother\" + 0.008*\"leav\" + 0.007*\"know\" + 0.007*\"tell\" + 0.007*\"son\"'),\n",
      " (12,\n",
      "  '0.048*\"love\" + 0.034*\"want\" + 0.033*\"know\" + 0.022*\"think\" + 0.017*\"thing\" '\n",
      "  '+ 0.014*\"dear\" + 0.014*\"tell\" + 0.012*\"sorri\" + 0.011*\"way\" + '\n",
      "  '0.011*\"happi\"'),\n",
      " (13,\n",
      "  '0.022*\"love\" + 0.012*\"hope\" + 0.012*\"help\" + 0.009*\"car\" + 0.009*\"want\" + '\n",
      "  '0.009*\"bank\" + 0.008*\"mari\" + 0.008*\"money\" + 0.008*\"smith\" + 0.007*\"re\"'),\n",
      " (14,\n",
      "  '0.023*\"good\" + 0.016*\"tri\" + 0.013*\"live\" + 0.012*\"mother\" + 0.011*\"leav\" + '\n",
      "  '0.011*\"god\" + 0.010*\"love\" + 0.009*\"know\" + 0.007*\"want\" + 0.007*\"money\"'),\n",
      " (15,\n",
      "  '0.014*\"know\" + 0.013*\"smith\" + 0.011*\"mari\" + 0.010*\"case\" + 0.010*\"box\" + '\n",
      "  '0.009*\"januari\" + 0.008*\"like\" + 0.008*\"year\" + 0.008*\"mr\" + 0.007*\"good\"'),\n",
      " (16,\n",
      "  '0.021*\"love\" + 0.013*\"go\" + 0.011*\"know\" + 0.011*\"good\" + 0.010*\"want\" + '\n",
      "  '0.010*\"suffer\" + 0.010*\"dear\" + 0.009*\"tell\" + 0.008*\"time\" + '\n",
      "  '0.008*\"forgiv\"'),\n",
      " (17,\n",
      "  '0.042*\"love\" + 0.018*\"life\" + 0.012*\"dear\" + 0.011*\"write\" + 0.009*\"mari\" + '\n",
      "  '0.009*\"want\" + 0.008*\"know\" + 0.008*\"mother\" + 0.008*\"wish\" + '\n",
      "  '0.007*\"cincinnati\"'),\n",
      " (18,\n",
      "  '0.018*\"get\" + 0.010*\"love\" + 0.010*\"pay\" + 0.010*\"come\" + 0.009*\"tell\" + '\n",
      "  '0.009*\"hope\" + 0.008*\"hous\" + 0.008*\"drink\" + 0.008*\"know\" + 0.008*\"think\"'),\n",
      " (19,\n",
      "  '0.014*\"mari\" + 0.013*\"love\" + 0.013*\"smith\" + 0.011*\"know\" + 0.010*\"sign\" + '\n",
      "  '0.009*\"day\" + 0.009*\"wife\" + 0.008*\"man\" + 0.008*\"children\" + 0.008*\"hope\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keywords in the 20 topics using the BOW corpus and 10 passes\n",
    "pprint(lda_model2.print_topics())\n",
    "doc_lda2 = lda_model2[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train LDA model using BOW, 20 topics, 20 passes\n",
    "lda_model3 = gensim.models.ldamodel.LdaModel(corpus=bow_corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           passes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.092*\"cincinnati\" + 0.075*\"burnet\" + 0.064*\"ave\" + 0.047*\"ohio\" + '\n",
      "  '0.024*\"smith\" + 0.020*\"hospit\" + 0.020*\"mr\" + 0.017*\"children\" + '\n",
      "  '0.015*\"notifi\" + 0.014*\"sign\"'),\n",
      " (1,\n",
      "  '0.029*\"know\" + 0.028*\"love\" + 0.026*\"good\" + 0.024*\"god\" + 0.021*\"way\" + '\n",
      "  '0.014*\"bless\" + 0.013*\"mother\" + 0.013*\"best\" + 0.012*\"sorri\" + '\n",
      "  '0.012*\"thing\"'),\n",
      " (2,\n",
      "  '0.025*\"blame\" + 0.013*\"know\" + 0.013*\"sign\" + 0.013*\"concern\" + '\n",
      "  '0.011*\"sheriff\" + 0.011*\"take\" + 0.011*\"suicid\" + 0.010*\"tire\" + '\n",
      "  '0.010*\"year\" + 0.009*\"time\"'),\n",
      " (3,\n",
      "  '0.019*\"care\" + 0.016*\"money\" + 0.012*\"car\" + 0.011*\"check\" + 0.011*\"pay\" + '\n",
      "  '0.009*\"work\" + 0.009*\"know\" + 0.008*\"like\" + 0.007*\"notifi\" + '\n",
      "  '0.007*\"happi\"'),\n",
      " (4,\n",
      "  '0.050*\"love\" + 0.032*\"good\" + 0.028*\"forgiv\" + 0.026*\"dear\" + 0.017*\"bye\" + '\n",
      "  '0.015*\"wife\" + 0.014*\"god\" + 0.014*\"way\" + 0.013*\"mom\" + 0.012*\"want\"'),\n",
      " (5,\n",
      "  '0.018*\"love\" + 0.012*\"mari\" + 0.012*\"day\" + 0.011*\"good\" + 0.010*\"thank\" + '\n",
      "  '0.009*\"dear\" + 0.009*\"money\" + 0.008*\"person\" + 0.008*\"live\" + '\n",
      "  '0.008*\"leav\"'),\n",
      " (6,\n",
      "  '0.013*\"love\" + 0.012*\"know\" + 0.012*\"want\" + 0.012*\"good\" + 0.009*\"go\" + '\n",
      "  '0.009*\"thing\" + 0.009*\"end\" + 0.009*\"hope\" + 0.008*\"children\" + '\n",
      "  '0.008*\"time\"'),\n",
      " (7,\n",
      "  '0.012*\"want\" + 0.011*\"mari\" + 0.011*\"go\" + 0.010*\"leav\" + 0.009*\"like\" + '\n",
      "  '0.009*\"think\" + 0.009*\"good\" + 0.008*\"know\" + 0.008*\"come\" + 0.008*\"year\"'),\n",
      " (8,\n",
      "  '0.028*\"love\" + 0.015*\"leav\" + 0.015*\"know\" + 0.015*\"want\" + 0.014*\"life\" + '\n",
      "  '0.014*\"tri\" + 0.012*\"live\" + 0.011*\"like\" + 0.011*\"help\" + 0.011*\"time\"'),\n",
      " (9,\n",
      "  '0.031*\"mari\" + 0.015*\"hope\" + 0.012*\"know\" + 0.010*\"smith\" + 0.010*\"wife\" + '\n",
      "  '0.009*\"love\" + 0.009*\"dear\" + 0.008*\"time\" + 0.008*\"write\" + 0.008*\"pay\"'),\n",
      " (10,\n",
      "  '0.023*\"pay\" + 0.022*\"smith\" + 0.018*\"mr\" + 0.018*\"money\" + 0.013*\"burnet\" + '\n",
      "  '0.013*\"owe\" + 0.012*\"bank\" + 0.011*\"wish\" + 0.010*\"cincinnati\" + '\n",
      "  '0.009*\"ave\"'),\n",
      " (11,\n",
      "  '0.013*\"want\" + 0.012*\"good\" + 0.010*\"live\" + 0.009*\"hope\" + 0.009*\"like\" + '\n",
      "  '0.008*\"leav\" + 0.008*\"mother\" + 0.008*\"longer\" + 0.007*\"come\" + '\n",
      "  '0.007*\"get\"'),\n",
      " (12,\n",
      "  '0.054*\"love\" + 0.036*\"know\" + 0.034*\"want\" + 0.021*\"think\" + 0.018*\"thing\" '\n",
      "  '+ 0.016*\"tell\" + 0.015*\"dear\" + 0.013*\"way\" + 0.013*\"sorri\" + 0.012*\"good\"'),\n",
      " (13,\n",
      "  '0.022*\"love\" + 0.011*\"hope\" + 0.009*\"car\" + 0.008*\"help\" + 0.008*\"re\" + '\n",
      "  '0.007*\"bank\" + 0.006*\"time\" + 0.006*\"forgiv\" + 0.006*\"bu\" + 0.006*\"decid\"'),\n",
      " (14,\n",
      "  '0.019*\"good\" + 0.016*\"tri\" + 0.013*\"mother\" + 0.012*\"live\" + 0.011*\"leav\" + '\n",
      "  '0.008*\"god\" + 0.008*\"love\" + 0.008*\"know\" + 0.007*\"children\" + '\n",
      "  '0.007*\"money\"'),\n",
      " (15,\n",
      "  '0.013*\"know\" + 0.011*\"mari\" + 0.011*\"smith\" + 0.011*\"case\" + 0.011*\"box\" + '\n",
      "  '0.009*\"januari\" + 0.008*\"year\" + 0.008*\"like\" + 0.007*\"mr\" + 0.007*\"care\"'),\n",
      " (16,\n",
      "  '0.018*\"love\" + 0.012*\"go\" + 0.011*\"suffer\" + 0.010*\"dear\" + 0.010*\"know\" + '\n",
      "  '0.009*\"good\" + 0.009*\"want\" + 0.008*\"god\" + 0.008*\"forgiv\" + 0.008*\"time\"'),\n",
      " (17,\n",
      "  '0.040*\"love\" + 0.019*\"life\" + 0.012*\"write\" + 0.010*\"dear\" + 0.009*\"mari\" + '\n",
      "  '0.009*\"mother\" + 0.008*\"wish\" + 0.008*\"want\" + 0.007*\"take\" + '\n",
      "  '0.007*\"thing\"'),\n",
      " (18,\n",
      "  '0.019*\"get\" + 0.011*\"come\" + 0.010*\"pay\" + 0.010*\"love\" + 0.009*\"drink\" + '\n",
      "  '0.009*\"hous\" + 0.009*\"tell\" + 0.009*\"hope\" + 0.009*\"know\" + 0.008*\"lose\"'),\n",
      " (19,\n",
      "  '0.014*\"mari\" + 0.012*\"smith\" + 0.010*\"day\" + 0.010*\"know\" + 0.010*\"sign\" + '\n",
      "  '0.009*\"man\" + 0.009*\"love\" + 0.008*\"wife\" + 0.008*\"come\" + 0.008*\"god\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keywords in the 20 topics using the BOW corpus with 20 passes\n",
    "pprint(lda_model3.print_topics())\n",
    "doc_lda3 = lda_model3[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove lemmatize/stem\n",
    "def preprocess2(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 2:\n",
    "            result.append(token)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3\n",
      "1 30\n",
      "2 Bill\n",
      "3 I\n",
      "4 Love\n",
      "5 M\n",
      "6 PM\n",
      "7 Saturday\n",
      "8 So\n",
      "9 Sweet\n",
      "10 all\n"
     ]
    }
   ],
   "source": [
    "#create dictionary of words without preprocessing\n",
    "dictionary2 = gensim.corpora.Dictionary(run1['tokenized_text'])\n",
    "count = 0\n",
    "for k, v in dictionary2.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus2 = [dictionary2.doc2bow(doc) for doc in run1['tokenized_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(23, 0.017417107462771197),\n",
      " (20, 0.0276747765945012),\n",
      " (11, 0.03088496593971787),\n",
      " (3, 0.03347347140912922),\n",
      " (17, 0.046831623893311206),\n",
      " (10, 0.05362203509858083),\n",
      " (12, 0.05981164526079127),\n",
      " (25, 0.0652722877678862),\n",
      " (21, 0.06785839884198139),\n",
      " (22, 0.06900116850299397),\n",
      " (2, 0.10363329152395466),\n",
      " (4, 0.11611760543679854),\n",
      " (5, 0.14808769195229632),\n",
      " (8, 0.1598449543354215),\n",
      " (13, 0.16265414320402255),\n",
      " (14, 0.17823308368439983),\n",
      " (0, 0.19076964267953514),\n",
      " (1, 0.22456110822242295),\n",
      " (16, 0.2266805921728961),\n",
      " (19, 0.2503108494717828),\n",
      " (6, 0.2739411067706694),\n",
      " (7, 0.28456669504453247),\n",
      " (18, 0.28456669504453247),\n",
      " (15, 0.3249628867145194),\n",
      " (9, 0.3547334901106432),\n",
      " (24, 0.4187599390795167)]\n"
     ]
    }
   ],
   "source": [
    "#determine the TF-IDF scores or weight of a word within a document without preprocessing\n",
    "from gensim import corpora, models\n",
    "tfidf2 = models.TfidfModel(bow_corpus2)\n",
    "corpus_tfidf2 = tfidf2[bow_corpus2]\n",
    "from pprint import pprint\n",
    "for doc in corpus_tfidf2:\n",
    "    pprint(sorted(doc, key = lambda x: x[1]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "urine ,Sweet ,fought ,lovely ,Saturday ,PM ,mercy ,hold ,30 ,3\n"
     ]
    }
   ],
   "source": [
    "#top 10 weighted words\n",
    "print(dictionary2[24] ,\",\" + dictionary2[9] ,\",\" + dictionary2[15] ,\",\" + dictionary2[18] ,\",\" + dictionary2[7] ,\",\" + dictionary2[6] ,\",\" + dictionary2[19] ,\",\" + dictionary2[16],\",\" + dictionary2[1],\",\" + dictionary2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "even ,end ,So ,M ,Love ,Bill ,on ,not ,you ,can\n"
     ]
    }
   ],
   "source": [
    "#other top weighted words\n",
    "print(dictionary2[14] ,\",\" + dictionary2[13] ,\",\" + dictionary2[8] ,\",\" + dictionary2[5] ,\",\" + dictionary2[4] ,\",\" + dictionary2[2] ,\",\" + dictionary2[22] ,\",\" + dictionary2[21],\",\" + dictionary2[25],\",\" + dictionary2[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train LDA model using BOW unprocessed, 10 topics, 10 passes\n",
    "lda_model_un = gensim.models.ldamodel.LdaModel(corpus=bow_corpus2,\n",
    "                                           id2word=dictionary2,\n",
    "                                           num_topics=10, \n",
    "                                           random_state=100,\n",
    "                                           passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.026*\"I\" + 0.022*\"a\" + 0.021*\"to\" + 0.016*\"the\" + 0.013*\"my\" + 0.012*\"of\" '\n",
      "  '+ 0.011*\"and\" + 0.009*\"be\" + 0.008*\"is\" + 0.008*\"in\"'),\n",
      " (1,\n",
      "  '0.070*\"I\" + 0.035*\"you\" + 0.034*\"to\" + 0.026*\"and\" + 0.021*\"the\" + '\n",
      "  '0.018*\"me\" + 0.017*\"my\" + 0.014*\"for\" + 0.014*\"have\" + 0.013*\"of\"'),\n",
      " (2,\n",
      "  '0.027*\"the\" + 0.027*\"and\" + 0.022*\"I\" + 0.021*\"to\" + 0.021*\"in\" + '\n",
      "  '0.018*\"of\" + 0.015*\"is\" + 0.015*\"my\" + 0.015*\"a\" + 0.010*\"you\"'),\n",
      " (3,\n",
      "  '0.013*\"to\" + 0.012*\"I\" + 0.011*\"my\" + 0.009*\"t\" + 0.008*\"it\" + 0.008*\"a\" + '\n",
      "  '0.006*\"for\" + 0.005*\"in\" + 0.005*\"the\" + 0.005*\"is\"'),\n",
      " (4,\n",
      "  '0.041*\"I\" + 0.036*\"the\" + 0.027*\"to\" + 0.019*\"and\" + 0.017*\"of\" + '\n",
      "  '0.017*\"you\" + 0.015*\"in\" + 0.015*\"that\" + 0.014*\"a\" + 0.014*\"for\"'),\n",
      " (5,\n",
      "  '0.042*\"to\" + 0.025*\"I\" + 0.022*\"you\" + 0.018*\"my\" + 0.017*\"the\" + '\n",
      "  '0.013*\"and\" + 0.011*\"of\" + 0.011*\"be\" + 0.011*\"have\" + 0.011*\"in\"'),\n",
      " (6,\n",
      "  '0.031*\"I\" + 0.027*\"the\" + 0.023*\"and\" + 0.018*\"to\" + 0.014*\"of\" + '\n",
      "  '0.012*\"was\" + 0.011*\"my\" + 0.011*\"me\" + 0.010*\"a\" + 0.009*\"in\"'),\n",
      " (7,\n",
      "  '0.033*\"J\" + 0.032*\"my\" + 0.030*\"to\" + 0.029*\"Cincinnati\" + 0.026*\"Burnet\" + '\n",
      "  '0.025*\"3333\" + 0.023*\"I\" + 0.021*\"Ave\" + 0.018*\"the\" + 0.017*\"and\"'),\n",
      " (8,\n",
      "  '0.038*\"the\" + 0.034*\"of\" + 0.027*\"and\" + 0.023*\"my\" + 0.022*\"I\" + '\n",
      "  '0.019*\"to\" + 0.015*\"in\" + 0.014*\"is\" + 0.010*\"as\" + 0.007*\"have\"'),\n",
      " (9,\n",
      "  '0.039*\"you\" + 0.032*\"to\" + 0.029*\"I\" + 0.022*\"the\" + 0.013*\"is\" + '\n",
      "  '0.013*\"and\" + 0.013*\"a\" + 0.013*\"my\" + 0.012*\"for\" + 0.012*\"it\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keywords in the 10 topics using the BOW corpus with 10 passes of unprocessing docs\n",
    "pprint(lda_model_un.print_topics())\n",
    "doc_lda_unprocessed = lda_model_un[bow_corpus2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [saturday, sweet, lovely, fought, hold, urine,...\n",
       "1       [know, forgive, know, keys, safty, deposit, bo...\n",
       "2       [newspaper, mrs, burnet, ave, cincinnati, answ...\n",
       "3       [police, words, sufficient, guess, let, know, ...\n",
       "4       [little, ann, remember, darling, daddy, sweete...\n",
       "                              ...                        \n",
       "1315    [landlord, doesnt, act, sister, mrs, smith, bu...\n",
       "1316                        [thing, called, crazy, loved]\n",
       "1317    [dirty, trick, play, want, live, longer, look,...\n",
       "1318    [xxxxxx, dear, girls, end, life, ambition, com...\n",
       "1319    [honey, swell, man, good, kid, brown, suit, wa...\n",
       "Name: text_2, Length: 1320, dtype: object"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use preprocess2 function \n",
    "processed_2 = run1['text_2'].map(preprocess2)\n",
    "processed_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 end\n",
      "1 fought\n",
      "2 hold\n",
      "3 love\n",
      "4 lovely\n",
      "5 mercy\n",
      "6 saturday\n",
      "7 sweet\n",
      "8 urine\n",
      "9 bank\n",
      "10 box\n"
     ]
    }
   ],
   "source": [
    "#create dictionary of words with preprocessing2\n",
    "dictionary3 = gensim.corpora.Dictionary(processed_2)\n",
    "count = 0\n",
    "for k, v in dictionary3.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus3 = [dictionary3.doc2bow(doc) for doc in processed_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 0.07781448518203195),\n",
      " (0, 0.20295714980476587),\n",
      " (2, 0.2776126727026534),\n",
      " (7, 0.2776126727026534),\n",
      " (5, 0.3111210148215803),\n",
      " (4, 0.35820818276575206),\n",
      " (6, 0.35820818276575206),\n",
      " (1, 0.4090582880688289),\n",
      " (8, 0.5271285762000236)]\n"
     ]
    }
   ],
   "source": [
    "#determine the TF-IDF scores or weight of a word within a document with preprocessing2\n",
    "from gensim import corpora, models\n",
    "tfidf3 = models.TfidfModel(bow_corpus3)\n",
    "corpus_tfidf3 = tfidf3[bow_corpus3]\n",
    "from pprint import pprint\n",
    "for doc in corpus_tfidf3:\n",
    "    pprint(sorted(doc, key = lambda x: x[1]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So ,30 ,PM ,Love ,M ,Saturday ,Bill ,3 ,I\n"
     ]
    }
   ],
   "source": [
    "#top 9 weighted words\n",
    "print(dictionary2[8] ,\",\" + dictionary2[1] ,\",\" + dictionary2[6] ,\",\" + dictionary2[4] ,\",\" + dictionary2[5] ,\",\" + dictionary2[7] ,\",\" + dictionary2[2] ,\",\" + dictionary2[0],\",\" + dictionary2[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train LDA model using BOW processed2, 10 topics, 10 passes\n",
    "lda_model_p2 = gensim.models.ldamodel.LdaModel(corpus=bow_corpus3,\n",
    "                                           id2word=dictionary3,\n",
    "                                           num_topics=10, \n",
    "                                           random_state=100,\n",
    "                                           passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.012*\"want\" + 0.010*\"love\" + 0.008*\"way\" + 0.008*\"know\" + 0.007*\"good\" + '\n",
      "  '0.006*\"like\" + 0.006*\"money\" + 0.006*\"wife\" + 0.005*\"things\" + 0.005*\"god\"'),\n",
      " (1,\n",
      "  '0.015*\"leave\" + 0.015*\"cincinnati\" + 0.014*\"burnet\" + 0.013*\"ave\" + '\n",
      "  '0.007*\"good\" + 0.007*\"want\" + 0.007*\"sheriff\" + 0.006*\"like\" + 0.006*\"ohio\" '\n",
      "  '+ 0.005*\"care\"'),\n",
      " (2,\n",
      "  '0.015*\"dear\" + 0.015*\"love\" + 0.014*\"want\" + 0.011*\"wife\" + 0.011*\"god\" + '\n",
      "  '0.009*\"way\" + 0.009*\"sorry\" + 0.009*\"know\" + 0.008*\"best\" + 0.008*\"hope\"'),\n",
      " (3,\n",
      "  '0.012*\"mrs\" + 0.010*\"want\" + 0.010*\"smith\" + 0.008*\"burnet\" + 0.008*\"hope\" '\n",
      "  '+ 0.006*\"cincinnati\" + 0.006*\"ave\" + 0.006*\"bank\" + 0.006*\"life\" + '\n",
      "  '0.006*\"know\"'),\n",
      " (4,\n",
      "  '0.014*\"love\" + 0.011*\"good\" + 0.010*\"know\" + 0.008*\"want\" + 0.007*\"time\" + '\n",
      "  '0.007*\"mary\" + 0.006*\"life\" + 0.006*\"best\" + 0.005*\"way\" + 0.005*\"loved\"'),\n",
      " (5,\n",
      "  '0.021*\"signed\" + 0.016*\"mary\" + 0.012*\"cincinnati\" + 0.011*\"smith\" + '\n",
      "  '0.008*\"wife\" + 0.008*\"testament\" + 0.008*\"burnet\" + 0.008*\"january\" + '\n",
      "  '0.008*\"concern\" + 0.006*\"life\"'),\n",
      " (6,\n",
      "  '0.040*\"love\" + 0.020*\"know\" + 0.015*\"good\" + 0.012*\"dear\" + 0.012*\"way\" + '\n",
      "  '0.012*\"life\" + 0.011*\"want\" + 0.010*\"think\" + 0.009*\"forgive\" + '\n",
      "  '0.008*\"time\"'),\n",
      " (7,\n",
      "  '0.011*\"god\" + 0.011*\"know\" + 0.010*\"hope\" + 0.009*\"cincinnati\" + '\n",
      "  '0.009*\"mary\" + 0.008*\"way\" + 0.008*\"love\" + 0.007*\"life\" + 0.007*\"good\" + '\n",
      "  '0.005*\"children\"'),\n",
      " (8,\n",
      "  '0.037*\"cincinnati\" + 0.034*\"burnet\" + 0.032*\"ave\" + 0.020*\"ohio\" + '\n",
      "  '0.017*\"smith\" + 0.014*\"mary\" + 0.014*\"children\" + 0.012*\"hospital\" + '\n",
      "  '0.012*\"good\" + 0.011*\"mrs\"'),\n",
      " (9,\n",
      "  '0.023*\"love\" + 0.016*\"forgive\" + 0.014*\"good\" + 0.012*\"dear\" + '\n",
      "  '0.010*\"mother\" + 0.010*\"sorry\" + 0.009*\"know\" + 0.009*\"cincinnati\" + '\n",
      "  '0.008*\"god\" + 0.008*\"hope\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keywords in the 10 topics using the BOW corpus with 10 passes of processed 2 docs\n",
    "pprint(lda_model_p2.print_topics())\n",
    "doc_lda_p2 = lda_model_p2[bow_corpus3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train LDA model using BOW processed2, 20 topics, 20 passes\n",
    "lda_model_p3 = gensim.models.ldamodel.LdaModel(corpus=bow_corpus3,\n",
    "                                           id2word=dictionary3,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           passes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.011*\"want\" + 0.009*\"love\" + 0.009*\"money\" + 0.006*\"wife\" + 0.006*\"way\" + '\n",
      "  '0.006*\"come\" + 0.005*\"care\" + 0.005*\"things\" + 0.005*\"job\" + 0.005*\"good\"'),\n",
      " (1,\n",
      "  '0.030*\"leave\" + 0.012*\"want\" + 0.012*\"sheriff\" + 0.011*\"good\" + '\n",
      "  '0.009*\"wife\" + 0.008*\"know\" + 0.008*\"cincinnati\" + 0.007*\"dear\" + '\n",
      "  '0.007*\"touch\" + 0.007*\"thanks\"'),\n",
      " (2,\n",
      "  '0.023*\"god\" + 0.019*\"love\" + 0.016*\"want\" + 0.013*\"dear\" + 0.013*\"wife\" + '\n",
      "  '0.011*\"good\" + 0.011*\"mother\" + 0.010*\"hope\" + 0.010*\"bless\" + '\n",
      "  '0.009*\"like\"'),\n",
      " (3,\n",
      "  '0.013*\"want\" + 0.012*\"know\" + 0.011*\"bank\" + 0.010*\"life\" + 0.007*\"paid\" + '\n",
      "  '0.007*\"money\" + 0.007*\"owe\" + 0.006*\"time\" + 0.006*\"think\" + 0.006*\"trunk\"'),\n",
      " (4,\n",
      "  '0.009*\"want\" + 0.006*\"money\" + 0.006*\"love\" + 0.006*\"right\" + '\n",
      "  '0.005*\"better\" + 0.005*\"good\" + 0.005*\"come\" + 0.005*\"help\" + 0.005*\"dad\" + '\n",
      "  '0.005*\"able\"'),\n",
      " (5,\n",
      "  '0.014*\"concern\" + 0.014*\"wife\" + 0.012*\"life\" + 0.011*\"signed\" + '\n",
      "  '0.008*\"want\" + 0.007*\"use\" + 0.007*\"mary\" + 0.007*\"beloved\" + 0.006*\"day\" + '\n",
      "  '0.006*\"shall\"'),\n",
      " (6,\n",
      "  '0.041*\"love\" + 0.017*\"dear\" + 0.015*\"know\" + 0.013*\"life\" + 0.012*\"forgive\" '\n",
      "  '+ 0.012*\"good\" + 0.010*\"want\" + 0.010*\"mother\" + 0.010*\"help\" + '\n",
      "  '0.008*\"time\"'),\n",
      " (7,\n",
      "  '0.012*\"god\" + 0.011*\"mary\" + 0.011*\"hope\" + 0.010*\"good\" + 0.009*\"let\" + '\n",
      "  '0.008*\"love\" + 0.008*\"forget\" + 0.008*\"way\" + 0.007*\"know\" + 0.007*\"want\"'),\n",
      " (8,\n",
      "  '0.017*\"children\" + 0.015*\"good\" + 0.013*\"mary\" + 0.012*\"know\" + '\n",
      "  '0.010*\"hospital\" + 0.010*\"dear\" + 0.009*\"time\" + 0.008*\"god\" + 0.008*\"let\" '\n",
      "  '+ 0.007*\"care\"'),\n",
      " (9,\n",
      "  '0.016*\"love\" + 0.014*\"children\" + 0.014*\"forgive\" + 0.011*\"hospital\" + '\n",
      "  '0.010*\"life\" + 0.010*\"good\" + 0.009*\"mother\" + 0.009*\"dear\" + 0.007*\"know\" '\n",
      "  '+ 0.007*\"sorry\"'),\n",
      " (10,\n",
      "  '0.013*\"want\" + 0.007*\"years\" + 0.006*\"box\" + 0.005*\"know\" + 0.005*\"good\" + '\n",
      "  '0.005*\"time\" + 0.005*\"got\" + 0.005*\"bank\" + 0.005*\"chest\" + '\n",
      "  '0.005*\"account\"'),\n",
      " (11,\n",
      "  '0.009*\"know\" + 0.008*\"want\" + 0.008*\"way\" + 0.007*\"years\" + 0.006*\"death\" + '\n",
      "  '0.006*\"like\" + 0.006*\"father\" + 0.005*\"look\" + 0.005*\"better\" + '\n",
      "  '0.005*\"mary\"'),\n",
      " (12,\n",
      "  '0.042*\"love\" + 0.027*\"know\" + 0.023*\"good\" + 0.021*\"way\" + 0.017*\"hope\" + '\n",
      "  '0.015*\"sorry\" + 0.014*\"think\" + 0.013*\"want\" + 0.011*\"dear\" + 0.010*\"like\"'),\n",
      " (13,\n",
      "  '0.087*\"cincinnati\" + 0.076*\"burnet\" + 0.062*\"ave\" + 0.044*\"ohio\" + '\n",
      "  '0.029*\"smith\" + 0.026*\"mrs\" + 0.015*\"signed\" + 0.015*\"notify\" + '\n",
      "  '0.012*\"wife\" + 0.011*\"phone\"'),\n",
      " (14,\n",
      "  '0.019*\"smith\" + 0.011*\"love\" + 0.009*\"good\" + 0.009*\"wonderful\" + '\n",
      "  '0.009*\"mary\" + 0.009*\"mrs\" + 0.008*\"dear\" + 0.008*\"hospital\" + '\n",
      "  '0.008*\"january\" + 0.007*\"children\"'),\n",
      " (15,\n",
      "  '0.038*\"mary\" + 0.020*\"love\" + 0.015*\"dear\" + 0.015*\"smith\" + 0.014*\"god\" + '\n",
      "  '0.012*\"good\" + 0.009*\"mother\" + 0.009*\"dad\" + 0.008*\"life\" + '\n",
      "  '0.007*\"forgive\"'),\n",
      " (16,\n",
      "  '0.013*\"smith\" + 0.007*\"love\" + 0.007*\"things\" + 0.007*\"mary\" + '\n",
      "  '0.006*\"people\" + 0.006*\"life\" + 0.005*\"family\" + 0.005*\"dear\" + '\n",
      "  '0.005*\"work\" + 0.004*\"old\"'),\n",
      " (17,\n",
      "  '0.010*\"good\" + 0.010*\"know\" + 0.009*\"tell\" + 0.008*\"years\" + 0.007*\"feel\" + '\n",
      "  '0.007*\"dear\" + 0.007*\"forgive\" + 0.007*\"mom\" + 0.006*\"life\" + '\n",
      "  '0.006*\"longer\"'),\n",
      " (18,\n",
      "  '0.014*\"god\" + 0.011*\"forgive\" + 0.007*\"signed\" + 0.007*\"xxx\" + '\n",
      "  '0.007*\"clothes\" + 0.006*\"want\" + 0.006*\"pay\" + 0.006*\"christian\" + '\n",
      "  '0.006*\"america\" + 0.005*\"check\"'),\n",
      " (19,\n",
      "  '0.011*\"house\" + 0.010*\"good\" + 0.009*\"mrs\" + 0.009*\"know\" + 0.008*\"mother\" '\n",
      "  '+ 0.007*\"awful\" + 0.007*\"paid\" + 0.006*\"love\" + 0.005*\"babies\" + '\n",
      "  '0.005*\"want\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keywords in the 20 topics using the BOW corpus with 20 passes of processed 2 docs\n",
    "pprint(lda_model_p3.print_topics())\n",
    "doc_lda_p2 = lda_model_p3[bow_corpus3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-intern]",
   "language": "python",
   "name": "conda-env-.conda-intern-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
